{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165 77760 243 320\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "import cv2 \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# path to the database - change it if needed\n",
    "path = 'D:\\\\Python\\\\MachineLearning\\\\MachineLearning\\\\week07\\\\data\\\\face_data\\\\face_data\\\\'\n",
    "ids = range(1, 16) # 15 people\n",
    "states = ['centerlight', 'glasses', 'happy', 'leftlight',\n",
    "        'noglasses', 'normal', 'rightlight','sad',\n",
    "        'sleepy', 'surprised', 'wink' ]\n",
    "prefix = 'subject'\n",
    "surfix = '.png' #file extension is png\n",
    "\n",
    "# open one picture to get the image's size\n",
    "fn = prefix + '01.' + states[0] + surfix\n",
    "im = cv2.imread(path + fn, 0)\n",
    "\n",
    "h = im.shape[0] # hight\n",
    "w = im.shape[1] # width\n",
    "\n",
    "D = h * w\n",
    "N = len(states)*15\n",
    "print(N, D, h, w)\n",
    "\n",
    "X = np.zeros((D, N))\n",
    "\n",
    "# collect all data\n",
    "count = 0\n",
    "state_labels = []\n",
    "\n",
    "# there are 15 people\n",
    "for person_id in range(1, 16):\n",
    "    for state in states:\n",
    "        state_labels.append(state)\n",
    "\n",
    "        # get name of each image file\n",
    "        fn = path + prefix + str(person_id).zfill(2) + '.' + state + surfix\n",
    "        \n",
    "        # open the file and read as grey image\n",
    "        tmp = cv2.imread(fn, cv2.IMREAD_GRAYSCALE)\n",
    "        # then add image to dataset X\n",
    "        X[:, count] = tmp.reshape(D)\n",
    "        count += 1\n",
    "\n",
    "# Tạo mảng y chứa nhãn cho mỗi mẫu dữ liệu\n",
    "y = np.array(state_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) Giảm số chiều dữ liệu xuống còn 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (77760, 165)\n",
      "Shape after PCA: (165, 100)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Tạo một đối tượng scaler\n",
    "scaler = StandardScaler()\n",
    "# Initialize PCA with n_components = min(n_samples, n_features)\n",
    "pca = PCA(100)\n",
    "\n",
    "X_scale = scaler.fit_transform(X.T)\n",
    "# Fit PCA to the data X\n",
    "X_pca = pca.fit_transform(X_scale)  # Transpose X to fit PCA (samples as rows, features as columns)\n",
    "\n",
    "print(\"Original shape:\", X.shape)\n",
    "print(\"Shape after PCA:\", X_pca.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Áp dụng các phương pháp phân loại nhiều lớp: Multinomial Logistic Regression, Naïve\n",
    "Bayes phù hợp và ANN (đã có code) để phân loại, tỷ lệ train:test là 0.7:0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Logistic Regression Accuracy: 0.16\n",
      "Naive Bayes Accuracy: 0.16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Multinomial Logistic Regression\n",
    "logreg = LogisticRegression(multi_class='multinomial', max_iter=5000)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_logreg = logreg.predict(X_test)\n",
    "accuracy_logreg = accuracy_score(y_test, y_pred_logreg)\n",
    "print(\"Multinomial Logistic Regression Accuracy:\", accuracy_logreg)\n",
    "\n",
    "# Naive Bayes\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred_nb = nb.predict(X_test)\n",
    "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_nb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(V):\n",
    "    e_V = np.exp(V - np.max(V, axis = 0, keepdims = True))\n",
    "    Z = e_V / e_V.sum(axis = 0)\n",
    "    return Z\n",
    "\n",
    "# cost or loss function\n",
    "def cost(Y, Yhat):\n",
    "    return -np.sum(Y*np.log(Yhat))/Y.shape[1]\n",
    "\n",
    "## One-hot coding\n",
    "from scipy import sparse\n",
    "def convert_labels(y, C = 11):\n",
    "    Y = sparse.coo_matrix((np.ones_like(y),(y, np.arange(len(y)))), shape = (C, len(y))).toarray()\n",
    "    return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n",
      "(165, 100)\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to integers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "d0 = 100\n",
    "d1 = h = 100 # size of hidden layer\n",
    "\n",
    "d2 = C = 11\n",
    "# initialize parameters randomly\n",
    "W1 = 0.01*np.random.randn(d0, d1)\n",
    "b1 = np.zeros((d1, 1))\n",
    "W2 = 0.01*np.random.randn(d1, d2)\n",
    "b2 = np.zeros((d2, 1))\n",
    "\n",
    "Y = convert_labels(y, C)\n",
    "N = X.shape[0]\n",
    "eta = 1 # learning rate\n",
    "print(W1.shape)\n",
    "print(X_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 165)\n",
      "(100, 165)\n",
      "(100, 165)\n",
      "(11, 165)\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    ## Feedforward\n",
    "    Z1 = np.dot(W1.T, X_pca.T) + b1\n",
    "    print(Z1.shape)\n",
    "    A1 = np.maximum(Z1, 0)\n",
    "    print(A1.shape)\n",
    "    Z2 = np.dot(W2.T, A1) + b2\n",
    "    print(Z1.shape)\n",
    "    Yhat = softmax(Z2)\n",
    "    print(Yhat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 2.417092\n",
      "iter 1000, loss: 0.333326\n",
      "iter 2000, loss: 0.174947\n",
      "iter 3000, loss: 0.132623\n",
      "iter 4000, loss: 0.113472\n"
     ]
    }
   ],
   "source": [
    "for i in range(5000):\n",
    "    ## Feedforward\n",
    "    Z1 = np.dot(W1.T, X_pca.T) + b1\n",
    "    A1 = np.maximum(Z1, 0)\n",
    "    Z2 = np.dot(W2.T, A1) + b2\n",
    "    Yhat = softmax(Z2)\n",
    "\n",
    "    # print loss after each 1000 iterations\n",
    "    if i %1000 == 0:\n",
    "        # compute the loss: average cross-entropy loss\n",
    "        loss = cost(Y, Yhat)\n",
    "        print(\"iter %d, loss: %f\" %(i, loss))\n",
    "\n",
    "    # backpropagation\n",
    "    E2 = (Yhat - Y )/N\n",
    "    dW2 = np.dot(A1, E2.T)\n",
    "    db2 = np.sum(E2, axis = 1, keepdims = True)\n",
    "    E1 = np.dot(W2, E2)\n",
    "    E1[Z1 <= 0] = 0 # gradient of ReLU\n",
    "    dW1 = np.dot(X_pca.T, E1.T)\n",
    "    db1 = np.sum(E1, axis = 1, keepdims = True)\n",
    "    # Gradient Descent update\n",
    "    W1 += -eta*dW1\n",
    "    b1 += -eta*db1\n",
    "    W2 += -eta*dW2\n",
    "    b2 += -eta*db2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 9.09 %\n"
     ]
    }
   ],
   "source": [
    "Z1 = np.dot(W1.T, X_pca.T) + b1\n",
    "A1 = np.maximum(Z1, 0)\n",
    "Z2 = np.dot(W2.T, A1) + b2\n",
    "predicted_class = np.argmax(Z2, axis=0)\n",
    "acc = 100*np.mean(predicted_class == y)\n",
    "print('training accuracy: %.2f %%' % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Load and preprocess the new images\n",
    "new_image_paths = ['D:\\\\Python\\\\MachineLearning\\\\MachineLearning\\\\week05\\\\data\\\\face_data\\\\face_data\\\\subject01.centerlight.png',\n",
    "                   'D:\\\\Python\\\\MachineLearning\\\\MachineLearning\\\\week05\\\\data\\\\face_data\\\\face_data\\\\subject01.glasses.png',\n",
    "                   'D:\\\\Python\\\\MachineLearning\\\\MachineLearning\\\\week05\\\\data\\\\face_data\\\\face_data\\\\subject01.happy.png',\n",
    "                   'D:\\\\Python\\\\MachineLearning\\\\MachineLearning\\\\week05\\\\data\\\\face_data\\\\face_data\\\\subject01.leftlight.png',\n",
    "                   'D:\\\\Python\\\\MachineLearning\\\\MachineLearning\\\\week05\\\\data\\\\face_data\\\\face_data\\\\subject01.normal.png']\n",
    "new_images = []\n",
    "for path in new_image_paths:\n",
    "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    # Resize image to match the dimensions of the original dataset\n",
    "    img_resized = cv2.resize(img, (243, 320))\n",
    "    new_images.append(img_resized.flatten())  # Flatten image to match the shape of data for PCA\n",
    "\n",
    "# Convert new images to numpy array\n",
    "X_new = np.array(new_images)\n",
    "\n",
    "# Transform new images using the PCA model trained on the original dataset\n",
    "X_new_pca = pca.transform(X_new)\n",
    "\n",
    "Z1 = np.dot(W1.T, X_new_pca.T) + b1\n",
    "A1 = np.maximum(Z1, 0)\n",
    "Z2 = np.dot(W2.T, A1) + b2\n",
    "predicted_class = np.argmax(Z2, axis=0)\n",
    "print(predicted_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chay tren trainning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 2.406574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FPT SHOP\\AppData\\Local\\Temp\\ipykernel_11748\\3476936258.py:2: RuntimeWarning: invalid value encountered in subtract\n",
      "  e_V = np.exp(V - np.max(V, axis = 0, keepdims = True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1000, loss: nan\n",
      "iter 2000, loss: nan\n",
      "iter 3000, loss: nan\n",
      "iter 4000, loss: nan\n",
      "training accuracy: 0.00 %\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to integers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "Y = label_encoder.fit_transform(y_train)\n",
    "\n",
    "d0 = 100\n",
    "d1 = h = 100 # size of hidden layer\n",
    "\n",
    "d2 = C = 11\n",
    "# initialize parameters randomly\n",
    "W1 = 0.01*np.random.randn(d0, d1)\n",
    "b1 = np.zeros((d1, 1))\n",
    "W2 = 0.01*np.random.randn(d1, d2)\n",
    "b2 = np.zeros((d2, 1))\n",
    "\n",
    "Y = convert_labels(Y, C)\n",
    "N = X_train.shape[0]\n",
    "eta = 1 # learning rate\n",
    "\n",
    "for i in range(5000):\n",
    "    ## Feedforward\n",
    "    Z1 = np.dot(W1.T, X_train.T) + b1\n",
    "    A1 = np.maximum(Z1, 0)\n",
    "    Z2 = np.dot(W2.T, A1) + b2\n",
    "    Yhat = softmax(Z2)\n",
    "\n",
    "    # print loss after each 1000 iterations\n",
    "    if i %1000 == 0:\n",
    "        # compute the loss: average cross-entropy loss\n",
    "        loss = cost(Y, Yhat)\n",
    "        print(\"iter %d, loss: %f\" %(i, loss))\n",
    "\n",
    "    # backpropagation\n",
    "    E2 = (Yhat - Y )/N\n",
    "    dW2 = np.dot(A1, E2.T)\n",
    "    db2 = np.sum(E2, axis = 1, keepdims = True)\n",
    "    E1 = np.dot(W2, E2)\n",
    "    E1[Z1 <= 0] = 0 # gradient of ReLU\n",
    "    dW1 = np.dot(X_train.T, E1.T)\n",
    "    db1 = np.sum(E1, axis = 1, keepdims = True)\n",
    "    # Gradient Descent update\n",
    "    W1 += -eta*dW1\n",
    "    b1 += -eta*db1\n",
    "    W2 += -eta*dW2\n",
    "    b2 += -eta*db2\n",
    "\n",
    "Z1 = np.dot(W1.T, X_test.T) + b1\n",
    "A1 = np.maximum(Z1, 0)\n",
    "Z2 = np.dot(W2.T, A1) + b2\n",
    "predicted_class = np.argmax(Z2, axis=0)\n",
    "acc = 100*np.mean(predicted_class == y_test)\n",
    "print('training accuracy: %.2f %%' % (acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
