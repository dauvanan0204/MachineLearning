{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 214 entries, 0 to 213\n",
      "Data columns (total 11 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Id_Number  214 non-null    int64  \n",
      " 1   RI         214 non-null    float64\n",
      " 2   Na         214 non-null    float64\n",
      " 3   Mg         214 non-null    float64\n",
      " 4   Al         214 non-null    float64\n",
      " 5   Si         214 non-null    float64\n",
      " 6   K          214 non-null    float64\n",
      " 7   Ca         214 non-null    float64\n",
      " 8   Ba         214 non-null    float64\n",
      " 9   Fe         214 non-null    float64\n",
      " 10  Type       214 non-null    int64  \n",
      "dtypes: float64(9), int64(2)\n",
      "memory usage: 18.5 KB\n",
      "None\n",
      "[1 2 3 5 6 7]\n",
      "Type\n",
      "2    76\n",
      "1    70\n",
      "7    29\n",
      "3    17\n",
      "5    13\n",
      "6     9\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "import numpy as np # linear algebra \n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) \n",
    "# change file_data to where did you put it! \n",
    "file_data = 'D:\\\\Python\\\\MachineLearning\\\\MachineLearning\\\\week04\\\\data\\\\glass.csv' \n",
    "glass_df = pd.read_csv(file_data) \n",
    "\n",
    "print(glass_df.info()) \n",
    "glass_df.drop(columns='Id_Number', inplace=True)\n",
    "glass_types = glass_df['Type'].unique() \n",
    "print(glass_types) \n",
    "print(glass_df['Type'].value_counts()) \n",
    "X = glass_df[glass_df.columns[:-1]] \n",
    "y = glass_df['Type'] \n",
    "y = np.asarray(y) - 1\n",
    "X = np.asarray(X)\n",
    "C = len(np.unique(y)) + 1# number of classes (for c = 1, 2, 3, 4, 5, 6, 7)\n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse  \n",
    "def convert_labels(y, C = C): \n",
    "    \"\"\" \n",
    "    convert 1d label to a matrix label: each column of this  \n",
    "    matrix coresponding to 1 element in y. In i-th column of Y,  \n",
    "    only one non-zeros element located in the y[i]-th position,  \n",
    "    and = 1 ex: y = [0, 2, 1, 0], and 3 classes then return \n",
    " \n",
    "            [[1, 0, 0, 1], \n",
    "             [0, 0, 1, 0], \n",
    "             [0, 1, 0, 0]] \n",
    "    \"\"\" \n",
    "    Y = sparse.coo_matrix((np.ones_like(y), (y, np.arange(len(y)))), shape = (C, len(y))).toarray() \n",
    "    return Y  \n",
    " \n",
    "# Y = convert_labels(y, C) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_stable(Z): \n",
    "    \"\"\"\n",
    "     Compute softmax values for each sets of scores in Z. \n",
    "    each column of Z is a set of score.     \n",
    "    \"\"\" \n",
    "    e_Z = np.exp(Z - np.max(Z, axis = 0, keepdims = True)) \n",
    "    A = e_Z / e_Z.sum(axis = 0) \n",
    "    return A \n",
    "def softmax(Z): \n",
    "    \"\"\" \n",
    "    #Compute softmax values for each sets of scores in V. \n",
    "    #each column of V is a set of score.     \n",
    "    \"\"\" \n",
    "    e_Z = np.exp(Z) \n",
    "    A = e_Z / e_Z.sum(axis = 0) \n",
    "    return A \n",
    "def softmax_regression(X, y, W_init, eta, tol = 1e-4, max_count = 10000): \n",
    "    W = [W_init]     \n",
    "    C = W_init.shape[1] \n",
    "    Y = convert_labels(y, C) \n",
    "    it = 0 \n",
    "    N = X.shape[1] \n",
    "    d = X.shape[0] \n",
    "     \n",
    "    count = 0 \n",
    "    check_w_after = 20 \n",
    "    while count < max_count: \n",
    "        # mix data  \n",
    "        mix_id = np.random.permutation(N) \n",
    "        for i in mix_id: \n",
    "            xi = X[:, i].reshape(d, 1) \n",
    "            yi = Y[:, i].reshape(C, 1) \n",
    "            ai = softmax_stable(np.dot(W[-1].T, xi)) \n",
    "            W_new = W[-1] + eta*xi.dot((yi - ai).T) \n",
    "            count += 1 \n",
    "            # stopping criteria \n",
    "            if count%check_w_after == 0:                 \n",
    "                if np.linalg.norm(W_new - W[-check_w_after]) < tol: \n",
    "                    return W \n",
    "            W.append(W_new) \n",
    "    return W \n",
    " \n",
    "# cost or loss function   \n",
    "def cost(X, Y, W): \n",
    "    A = softmax(W.T.dot(X)) \n",
    "    return -np.sum(Y*np.log(A)) \n",
    " \n",
    "# Predict that X belong to which class (1..C now indexed as 0..C-1 )  \n",
    "def pred(W, X): \n",
    "    \"\"\" \n",
    "    predict output of each columns of X \n",
    "    Class of each x_i is determined by location of max probability \n",
    "    Note that class are indexed by [0, 1, 2, ...., C-1] \n",
    "    \"\"\" \n",
    "    A = softmax_stable(W.T.dot(X)) \n",
    "    return np.argmax(A, axis = 0) \n",
    " \n",
    "# W[-1] is the solution, W is all history of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.64294324e-01  1.20107946e-02  1.72252368e+00  4.13084221e-01\n",
      "   3.75271199e-02 -4.86146953e-01 -1.83217333e-01]\n",
      " [-1.39576832e+01 -3.34027256e+01  7.40649440e+00  3.31678507e-01\n",
      "  -1.17237435e+01  2.25876033e+01  3.30215685e+01]\n",
      " [ 7.05271655e+01  2.19410382e+01  2.18178938e+01  1.54826732e+00\n",
      "  -3.62700915e+01 -1.85186700e+01 -6.37141328e+01]\n",
      " [-3.41133165e+01  3.69099040e+00 -7.35995581e+00 -1.37483598e+00\n",
      "   1.68025092e+01 -4.30833115e+00  2.53664297e+01]\n",
      " [ 3.44454224e+00  1.57007271e+00 -1.64631035e+00 -4.97695617e-01\n",
      "   2.87267043e-01 -2.26060718e+00 -2.15301427e+00]\n",
      " [-6.39393576e+00  5.53659565e+00 -3.15592871e+00 -6.97631909e-01\n",
      "   3.01154876e+01 -9.07713318e+00 -9.24909563e+00]\n",
      " [ 1.75046996e+00  1.33904069e+01 -3.03260272e+00 -9.97733259e-01\n",
      "   1.42307642e+01  5.98845028e-01 -2.57132869e+01]\n",
      " [-1.24607525e+01 -1.15247482e+01 -5.41656749e+00  6.45856757e-02\n",
      "  -6.51707398e-01 -6.51707701e+00  3.14037855e+01]\n",
      " [-1.43671341e+00  5.04394425e+00  1.14927055e+00 -2.35651128e+00\n",
      "   2.14249580e-01 -5.74249968e-01 -1.80804610e+00]]\n"
     ]
    }
   ],
   "source": [
    "eta = .05  \n",
    "d = X.T.shape[0]\n",
    "W_init = np.random.randn(X.T.shape[0], C) \n",
    "W = softmax_regression(X.T, y, W_init, eta) \n",
    "print(W[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:\n",
      "[[-6.69916566e-03  8.82041324e-01  5.29977991e-02  8.33019900e-01\n",
      "  -1.57328039e-01 -1.85765092e+00 -4.83550916e-03]\n",
      " [-1.08637570e+01 -3.00109242e+01  4.70206226e+00 -1.68414164e-01\n",
      "  -1.38761719e+01  1.56396291e+01  3.56986900e+01]\n",
      " [ 6.96856460e+01  1.76457114e+01  2.60640350e+01  9.05514904e-01\n",
      "  -3.35181549e+01 -1.73095622e+01 -6.34021720e+01]\n",
      " [-3.06139572e+01  4.31310726e+00 -6.93091291e+00  4.66218180e-01\n",
      "   1.57614905e+01 -6.53887398e-01  2.17013611e+01]\n",
      " [ 5.38888273e-01  4.35578815e+00 -1.11505674e+00 -3.78372689e-01\n",
      "   1.28486358e+00 -2.10185322e+00 -8.15843904e-01]\n",
      " [-7.83642747e+00  2.64558906e+00 -5.26172486e-01 -8.29099041e-01\n",
      "   2.30741412e+01 -1.08309090e+01 -6.85346373e+00]\n",
      " [-4.40493758e+00  2.04055978e+01 -3.01880447e+00  2.18554207e+00\n",
      "   1.48757697e+01  4.67814702e+00 -3.33342643e+01]\n",
      " [-1.32231970e+01 -9.62817928e+00 -5.93078282e+00 -9.93465560e-01\n",
      "   1.81519705e+00 -3.55768584e+00  3.44688282e+01]\n",
      " [-2.22920194e+00  4.93679723e+00  1.99659530e-01  5.04988024e-01\n",
      "  -2.03328207e+00 -1.14279964e+00 -2.19119678e+00]]\n",
      "[1 6 1 6 1 1 1 1 1 1 6 1 1 1 1 1 1 1 1 6 1 6 6 6 1 1 1 1 1 1 1 1 1 1 1 6 4\n",
      " 1 1 1 1 6 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Accuracy Score: \n",
      "0.5370370370370371\n",
      "Confusion Matrix:\n",
      "[[ 0 14  0  0  0  0]\n",
      " [ 0 21  0  0  0  0]\n",
      " [ 0  4  0  0  0  0]\n",
      " [ 0  3  0  1  0  0]\n",
      " [ 0  1  0  0  0  2]\n",
      " [ 0  1  0  0  0  7]]\n"
     ]
    }
   ],
   "source": [
    "import sklearn \n",
    "# Applying logistic sigmoid regression to find coefficients\n",
    "w_init = np.random.randn(X_train.T.shape[0], C)\n",
    "eta = 0.05\n",
    "w = softmax_regression(X_train.T, y_train.T, w_init, eta)\n",
    "\n",
    "# Print coefficients\n",
    "print(\"Coefficients:\")\n",
    "print(w[-1])\n",
    "\n",
    "Y_predict = pred(w[-1], X_test.T)\n",
    "print(Y_predict)\n",
    "\n",
    "# for accuracy \n",
    "from sklearn.metrics import accuracy_score \n",
    "print(\"Accuracy Score: \")\n",
    "print(accuracy_score(y_test,Y_predict)) \n",
    " \n",
    "# for confusion matrix \n",
    "from sklearn.metrics import confusion_matrix \n",
    "cm=confusion_matrix(y_test,Y_predict) \n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code bằng thư viện Scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: \n",
      "0.7037037037037037\n",
      "Confusion Matrix:\n",
      "[[12  2  0  0  0  0]\n",
      " [ 4 16  0  0  1  0]\n",
      " [ 2  2  0  0  0  0]\n",
      " [ 0  2  0  2  0  0]\n",
      " [ 0  1  0  0  0  2]\n",
      " [ 0  0  0  0  0  8]]\n",
      "Coefficients:\n",
      "[[-2.87487924e-02 -6.13111726e-01  1.20719919e+00 -1.84856027e+00\n",
      "   9.62076558e-02 -2.93672752e-01  1.90300290e-01  1.58117744e-01\n",
      "  -2.00034843e-01]\n",
      " [ 4.72709370e-02 -3.34480263e-01  5.25823844e-01  4.94128449e-01\n",
      "  -1.81923594e-03  3.10803042e-01  4.27061281e-01  2.11775089e-01\n",
      "   7.13063884e-01]\n",
      " [ 1.18549126e-02  1.79388352e-01  1.05268172e+00 -3.25097187e-01\n",
      "  -1.13191709e-01 -2.12974647e-01  3.78947660e-01 -2.16235877e-01\n",
      "  -9.90412204e-02]\n",
      " [ 2.11046342e-02 -6.30703698e-01 -6.26792161e-01  1.76844819e+00\n",
      "   7.01361985e-02  9.28893691e-01  1.37464991e-01 -7.53715188e-02\n",
      "  -1.36747669e-01]\n",
      " [-4.51379486e-02  8.01658078e-01 -7.06980035e-01 -3.57486289e-01\n",
      "  -7.47868102e-02 -1.12592211e+00 -3.27870915e-01 -7.04066110e-01\n",
      "  -1.69079183e-01]\n",
      " [-6.34374271e-03  5.97249257e-01 -1.45193256e+00  2.68567106e-01\n",
      "   2.34539009e-02  3.92872778e-01 -8.05903306e-01  6.25780674e-01\n",
      "  -1.08160969e-01]]\n",
      "Predict:\n",
      "[0 6 0 6 1 1 0 0 1 1 6 1 0 1 1 4 6 0 1 6 1 6 6 6 1 1 0 0 1 0 1 1 1 1 0 6 4\n",
      " 0 0 1 1 6 0 1 0 5 1 1 1 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import sklearn \n",
    "#from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "# Call to Logistic Regression Model - SAG: solving is based on Stochastic Average Gradient \n",
    "lorg=LogisticRegression(multi_class='multinomial',solver='sag', max_iter=5000) \n",
    "# and train model by Training Dataset \n",
    "lorg.fit(X_train,y_train)  \n",
    " \n",
    "# Then Predict the Test data \n",
    "Y_pred=lorg.predict(X_test) \n",
    " \n",
    "# for accuracy \n",
    "from sklearn.metrics import accuracy_score \n",
    "print(\"Accuracy Score: \")\n",
    "print(accuracy_score(y_test,Y_pred)) \n",
    " \n",
    "# for confusion matrix \n",
    "from sklearn.metrics import confusion_matrix \n",
    "cm=confusion_matrix(y_test,Y_pred) \n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm) \n",
    "print(\"Coefficients:\")\n",
    "print(lorg.coef_)\n",
    "y_predict = lorg.predict(X_test)\n",
    "print(\"Predict:\")\n",
    "print(y_predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
